{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"sbi \u00b6 sbi : A Python toolbox to perform simulation-based inference using density-estimation approaches. The focus of sbi is Sequential Neural Posterior Estimation (SNPE). In SNPE, a neural network is trained to perform Bayesian inference on simulated data. To see illustrations of SNPE on canonical problems in neuroscience, read our preprint: Training deep neural density estimators to identify mechanistic models of neural dynamics . To learn more about the general motivation behind simulation-based inference, and algorithms included in sbi , keep on reading. We are working on making tutorials available. Motivation and approach \u00b6 Many areas of science and engineering make extensive use of complex, stochastic, numerical simulations to describe the structure and dynamics of the processes being investigated. A key challenge in simulation-based science is linking simulation models to empirical data: Bayesian inference provides a general and powerful framework for identifying the set of parameters which are consistent both with empirical data and prior knowledge. One of the key quantities required for statistical inference, the likelihood of observed data given parameters, \\(\\mathcal{L}(\\theta) = p(x_o|\\theta)\\) , is typically intractable for simulation-based models, rendering conventional statistical approaches inapplicable. Sequential Neural Posterior Estimation (SNPE) is a powerful machine-learning technique to address this problem. Goal: Algorithmically identify mechanistic models which are consistent with data. SNPE takes three inputs: A candidate mechanistic model, prior knowledge or constraints on model parameters, and data (or summary statistics). SNPE proceeds by: sampling parameters from the prior and simulating synthetic datasets from these parameters, and using a deep density estimation neural network to learn the (probabilistic) association between data (or data features) and underlying parameters, i.e. to learn statistical inference from simulated data. This density estimation network is then applied to empirical data to derive the full space of parameters consistent with the data and the prior, i.e. the posterior distribution. High posterior probability is assigned to parameters which are consistent with both the data and the prior, low probability to inconsistent parameters. If needed, an initial estimate of the posterior can be used to adaptively generate additional informative simulations. Publications \u00b6 Algorithms included in sbi were published in the following papers, which provide additional information: Fast \u03b5-free Inference of Simulation Models with Bayesian Conditional Density Estimation by Papamakarios & Murray (NeurIPS 2016) [PDF] [BibTeX] Flexible statistical inference for mechanistic models of neural dynamics by Lueckmann, Goncalves, Bassetto, \u00d6cal, Nonnenmacher & Macke (NeurIPS 2017) [PDF] [BibTeX] Automatic posterior transformation for likelihood-free inference by Greenberg, Nonnenmacher & Macke (ICML 2019) [PDF] [BibTeX] On Contrastive Learning for Likelihood-free Inference by Durkan, Murray, Papamakarios (arXiv 2020) [PDF] We refer to these algorithms as SNPE-A, SNPE-B, and SNPE-C/APT, respectively. As an alternative to directly estimating the posterior on parameters given data, it is also possible to estimate the likelihood of data given parameters, and then subsequently draw posterior samples using MCMC ( Papamakarios, Sterratt & Murray, 2019 1 , Lueckmann, Karaletsos, Bassetto, Macke, 2019 ). Depending on the problem, approximating the likelihood can be more or less effective than SNPE techniques. See Cranmer, Brehmer, Louppe (2019) for a recent review on simulation-based inference and our recent preprint Training deep neural density estimators to identify mechanistic models of neural dynamics (Goncalves et al., 2019) for applications to canonical problems in neuroscience. Code for SNL is available from the original repository or as a python 3 package . \u21a9","title":"Home"},{"location":"#sbi","text":"sbi : A Python toolbox to perform simulation-based inference using density-estimation approaches. The focus of sbi is Sequential Neural Posterior Estimation (SNPE). In SNPE, a neural network is trained to perform Bayesian inference on simulated data. To see illustrations of SNPE on canonical problems in neuroscience, read our preprint: Training deep neural density estimators to identify mechanistic models of neural dynamics . To learn more about the general motivation behind simulation-based inference, and algorithms included in sbi , keep on reading. We are working on making tutorials available.","title":"sbi"},{"location":"#motivation-and-approach","text":"Many areas of science and engineering make extensive use of complex, stochastic, numerical simulations to describe the structure and dynamics of the processes being investigated. A key challenge in simulation-based science is linking simulation models to empirical data: Bayesian inference provides a general and powerful framework for identifying the set of parameters which are consistent both with empirical data and prior knowledge. One of the key quantities required for statistical inference, the likelihood of observed data given parameters, \\(\\mathcal{L}(\\theta) = p(x_o|\\theta)\\) , is typically intractable for simulation-based models, rendering conventional statistical approaches inapplicable. Sequential Neural Posterior Estimation (SNPE) is a powerful machine-learning technique to address this problem. Goal: Algorithmically identify mechanistic models which are consistent with data. SNPE takes three inputs: A candidate mechanistic model, prior knowledge or constraints on model parameters, and data (or summary statistics). SNPE proceeds by: sampling parameters from the prior and simulating synthetic datasets from these parameters, and using a deep density estimation neural network to learn the (probabilistic) association between data (or data features) and underlying parameters, i.e. to learn statistical inference from simulated data. This density estimation network is then applied to empirical data to derive the full space of parameters consistent with the data and the prior, i.e. the posterior distribution. High posterior probability is assigned to parameters which are consistent with both the data and the prior, low probability to inconsistent parameters. If needed, an initial estimate of the posterior can be used to adaptively generate additional informative simulations.","title":"Motivation and approach"},{"location":"#publications","text":"Algorithms included in sbi were published in the following papers, which provide additional information: Fast \u03b5-free Inference of Simulation Models with Bayesian Conditional Density Estimation by Papamakarios & Murray (NeurIPS 2016) [PDF] [BibTeX] Flexible statistical inference for mechanistic models of neural dynamics by Lueckmann, Goncalves, Bassetto, \u00d6cal, Nonnenmacher & Macke (NeurIPS 2017) [PDF] [BibTeX] Automatic posterior transformation for likelihood-free inference by Greenberg, Nonnenmacher & Macke (ICML 2019) [PDF] [BibTeX] On Contrastive Learning for Likelihood-free Inference by Durkan, Murray, Papamakarios (arXiv 2020) [PDF] We refer to these algorithms as SNPE-A, SNPE-B, and SNPE-C/APT, respectively. As an alternative to directly estimating the posterior on parameters given data, it is also possible to estimate the likelihood of data given parameters, and then subsequently draw posterior samples using MCMC ( Papamakarios, Sterratt & Murray, 2019 1 , Lueckmann, Karaletsos, Bassetto, Macke, 2019 ). Depending on the problem, approximating the likelihood can be more or less effective than SNPE techniques. See Cranmer, Brehmer, Louppe (2019) for a recent review on simulation-based inference and our recent preprint Training deep neural density estimators to identify mechanistic models of neural dynamics (Goncalves et al., 2019) for applications to canonical problems in neuroscience. Code for SNL is available from the original repository or as a python 3 package . \u21a9","title":"Publications"},{"location":"credits/","text":"Credits \u00b6 Code \u00b6 This code builds heavily on previous work by Conor Durkan , George Papamakarios and Artur Bekasov . Relevant repositories include bayesiains/nsf and conormdurkan/lfi . Algorithms \u00b6 sbi implements algorithms from the following papers: Fast \u03b5-free Inference of Simulation Models with Bayesian Conditional Density Estimation by Papamakarios & Murray (NeurIPS 2016) [PDF] [BibTeX] Flexible statistical inference for mechanistic models of neural dynamics by Lueckmann, Goncalves, Bassetto, \u00d6cal, Nonnenmacher & Macke (NeurIPS 2017) [PDF] [BibTeX] Automatic posterior transformation for likelihood-free inference by Greenberg, Nonnenmacher & Macke (ICML 2019) [PDF] [BibTeX] On Contrastive Learning for Likelihood-free Inference by Durkan, Murray, Papamakarios (arXiv 2020) [PDF] We refer to these algorithms as SNPE-A, SNPE-B, and SNPE-C/APT, respectively.","title":"Credits"},{"location":"credits/#credits","text":"","title":"Credits"},{"location":"credits/#code","text":"This code builds heavily on previous work by Conor Durkan , George Papamakarios and Artur Bekasov . Relevant repositories include bayesiains/nsf and conormdurkan/lfi .","title":"Code"},{"location":"credits/#algorithms","text":"sbi implements algorithms from the following papers: Fast \u03b5-free Inference of Simulation Models with Bayesian Conditional Density Estimation by Papamakarios & Murray (NeurIPS 2016) [PDF] [BibTeX] Flexible statistical inference for mechanistic models of neural dynamics by Lueckmann, Goncalves, Bassetto, \u00d6cal, Nonnenmacher & Macke (NeurIPS 2017) [PDF] [BibTeX] Automatic posterior transformation for likelihood-free inference by Greenberg, Nonnenmacher & Macke (ICML 2019) [PDF] [BibTeX] On Contrastive Learning for Likelihood-free Inference by Durkan, Murray, Papamakarios (arXiv 2020) [PDF] We refer to these algorithms as SNPE-A, SNPE-B, and SNPE-C/APT, respectively.","title":"Algorithms"},{"location":"install/","text":"Installation \u00b6 Quick start \u00b6 Clone the repo and install all the dependencies using the environment.yml file to create a conda environment: conda env create -f environment.yml . If you already have an sbi environment and want to refresh dependencies, just run conda env update -f environment.yml --prune . Alternatively, you can install via setup.py using pip install -e \".[dev]\" (the dev flag installs development and testing dependencies). git clone https://github.com/mackelab/sbi.git cd sbi pip install -e \".dev\"","title":"Installation"},{"location":"install/#installation","text":"","title":"Installation"},{"location":"install/#quick-start","text":"Clone the repo and install all the dependencies using the environment.yml file to create a conda environment: conda env create -f environment.yml . If you already have an sbi environment and want to refresh dependencies, just run conda env update -f environment.yml --prune . Alternatively, you can install via setup.py using pip install -e \".[dev]\" (the dev flag installs development and testing dependencies). git clone https://github.com/mackelab/sbi.git cd sbi pip install -e \".dev\"","title":"Quick start"},{"location":"reference/","text":"API Reference \u00b6 Inference \u00b6 sbi.inference.snpe.snpe_a.SnpeA \u00b6 __init__ ( self , simulator , prior , true_observation , num_pilot_samples = 100 , density_estimator = 'maf' , use_combined_loss = False , z_score_obs = True , simulation_batch_size = 1 , retrain_from_scratch_each_round = False , discard_prior_samples = False , summary_writer = None , device = None ) special SNPE-A Implementation of Fast epsilon-free Inference of Simulation Models with Bayesian Conditional Density Estimation by Papamakarios et al., NeurIPS 2016, https://arxiv.org/abs/1605.06376 Parameters: Name Type Description Default num_pilot_samples number of simulations that are run when instantiating an object. Used to z-score the observations. 100 density_estimator neural density estimator 'maf' calibration_kernel a function to calibrate the context required z_score_obs whether to z-score the data features x True use_combined_loss whether to jointly neural_net prior samples using maximum likelihood. Useful to prevent density leaking when using box uniform priors. False retrain_from_scratch_each_round whether to retrain the conditional density estimator for the posterior from scratch each round. False discard_prior_samples whether to discard prior samples from round two onwards. False Source code in sbi/inference/snpe/snpe_a.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def __init__ ( self , simulator , prior , true_observation , num_pilot_samples = 100 , density_estimator = \"maf\" , use_combined_loss = False , z_score_obs = True , simulation_batch_size : int = 1 , retrain_from_scratch_each_round = False , discard_prior_samples = False , summary_writer = None , device = None , ): \"\"\"SNPE-A Implementation of _Fast epsilon-free Inference of Simulation Models with Bayesian Conditional Density Estimation_ by Papamakarios et al., NeurIPS 2016, https://arxiv.org/abs/1605.06376 Args: num_pilot_samples: number of simulations that are run when instantiating an object. Used to z-score the observations. density_estimator: neural density estimator calibration_kernel: a function to calibrate the context z_score_obs: whether to z-score the data features x use_combined_loss: whether to jointly neural_net prior samples using maximum likelihood. Useful to prevent density leaking when using box uniform priors. retrain_from_scratch_each_round: whether to retrain the conditional density estimator for the posterior from scratch each round. discard_prior_samples: whether to discard prior samples from round two onwards. \"\"\" super ( SnpeA , self ) . __init__ ( simulator = simulator , prior = prior , true_observation = true_observation , num_pilot_samples = num_pilot_samples , density_estimator = density_estimator , use_combined_loss = use_combined_loss , z_score_obs = z_score_obs , simulation_batch_size = simulation_batch_size , retrain_from_scratch_each_round = retrain_from_scratch_each_round , discard_prior_samples = discard_prior_samples , device = device , ) sbi.inference.snpe.snpe_b.SnpeB \u00b6 __init__ ( self , simulator , prior , true_observation , num_pilot_samples = 100 , density_estimator = 'maf' , calibration_kernel = None , use_combined_loss = False , z_score_obs = True , simulation_batch_size = 1 , retrain_from_scratch_each_round = False , discard_prior_samples = False , summary_writer = None , device = None ) special Implementation of Flexible statistical inference for mechanistic models of neural dynamics by Lueckmann et al., NeurIPS 2017, https://arxiv.org/abs/1711.01861 Parameters: Name Type Description Default num_pilot_samples number of simulations that are run when instantiating an object. Used to z-score the observations. 100 density_estimator neural density estimator 'maf' calibration_kernel a function to calibrate the context None z_score_obs whether to z-score the data features x True use_combined_loss whether to jointly neural_net prior samples using maximum likelihood. Useful to prevent density leaking when using box uniform priors. False retrain_from_scratch_each_round whether to retrain the conditional density estimator for the posterior from scratch each round. False discard_prior_samples whether to discard prior samples from round two onwards. False Source code in sbi/inference/snpe/snpe_b.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def __init__ ( self , simulator , prior , true_observation , num_pilot_samples = 100 , density_estimator = \"maf\" , calibration_kernel = None , use_combined_loss = False , z_score_obs = True , simulation_batch_size : int = 1 , retrain_from_scratch_each_round = False , discard_prior_samples = False , summary_writer = None , device = None , ): \"\"\" Implementation of __Flexible statistical inference for mechanistic models of neural dynamics__ by Lueckmann et al., NeurIPS 2017, https://arxiv.org/abs/1711.01861 Args: num_pilot_samples: number of simulations that are run when instantiating an object. Used to z-score the observations. density_estimator: neural density estimator calibration_kernel: a function to calibrate the context z_score_obs: whether to z-score the data features x use_combined_loss: whether to jointly neural_net prior samples using maximum likelihood. Useful to prevent density leaking when using box uniform priors. retrain_from_scratch_each_round: whether to retrain the conditional density estimator for the posterior from scratch each round. discard_prior_samples: whether to discard prior samples from round two onwards. \"\"\" super ( SnpeB , self ) . __init__ ( simulator = simulator , prior = prior , true_observation = true_observation , num_pilot_samples = num_pilot_samples , density_estimator = density_estimator , calibration_kernel = calibration_kernel , use_combined_loss = use_combined_loss , z_score_obs = z_score_obs , simulation_batch_size = simulation_batch_size , retrain_from_scratch_each_round = retrain_from_scratch_each_round , discard_prior_samples = discard_prior_samples , device = device , ) sbi.inference.snpe.snpe_c.SnpeC \u00b6 __init__ ( self , simulator , prior , true_observation , num_atoms =- 1 , num_pilot_samples = 100 , density_estimator = None , calibration_kernel = None , use_combined_loss = False , z_score_obs = True , simulation_batch_size = 1 , retrain_from_scratch_each_round = False , discard_prior_samples = False , summary_writer = None , device = None , sample_with_mcmc = False , mcmc_method = 'slice-np' ) special SNPE-C / APT Implementation of Automatic Posterior Transformation for Likelihood-free Inference by Greenberg et al., ICML 2019, https://arxiv.org/abs/1905.07488 Parameters: Name Type Description Default num_pilot_samples number of simulations that are run when instantiating an object. Used to z-score the observations. 100 density_estimator neural density estimator None calibration_kernel a function to calibrate the context None z_score_obs whether to z-score the data features x True use_combined_loss whether to jointly neural_net prior samples using maximum likelihood. Useful to prevent density leaking when using box uniform priors. False retrain_from_scratch_each_round whether to retrain the conditional density estimator for the posterior from scratch each round. False discard_prior_samples whether to discard prior samples from round two onwards. False num_atoms int Number of atoms to use for classification. If -1, use all other parameters in minibatch. -1 Source code in sbi/inference/snpe/snpe_c.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def __init__ ( self , simulator , prior , true_observation , num_atoms =- 1 , num_pilot_samples = 100 , density_estimator = None , calibration_kernel = None , use_combined_loss = False , z_score_obs = True , simulation_batch_size : int = 1 , retrain_from_scratch_each_round = False , discard_prior_samples = False , summary_writer = None , device = None , sample_with_mcmc = False , mcmc_method = \"slice-np\" , ): \"\"\"SNPE-C / APT Implementation of _Automatic Posterior Transformation for Likelihood-free Inference_ by Greenberg et al., ICML 2019, https://arxiv.org/abs/1905.07488 Args: num_pilot_samples: number of simulations that are run when instantiating an object. Used to z-score the observations. density_estimator: neural density estimator calibration_kernel: a function to calibrate the context z_score_obs: whether to z-score the data features x use_combined_loss: whether to jointly neural_net prior samples using maximum likelihood. Useful to prevent density leaking when using box uniform priors. retrain_from_scratch_each_round: whether to retrain the conditional density estimator for the posterior from scratch each round. discard_prior_samples: whether to discard prior samples from round two onwards. num_atoms: int Number of atoms to use for classification. If -1, use all other parameters in minibatch. \"\"\" super ( SnpeC , self ) . __init__ ( simulator = simulator , prior = prior , true_observation = true_observation , num_pilot_samples = num_pilot_samples , density_estimator = density_estimator , calibration_kernel = calibration_kernel , use_combined_loss = use_combined_loss , z_score_obs = z_score_obs , simulation_batch_size = simulation_batch_size , retrain_from_scratch_each_round = retrain_from_scratch_each_round , discard_prior_samples = discard_prior_samples , device = device , sample_with_mcmc = sample_with_mcmc , mcmc_method = mcmc_method , ) assert isinstance ( num_atoms , int ), \"Number of atoms must be an integer.\" self . _num_atoms = num_atoms sbi.inference.snl.snl.SNL \u00b6 __call__ ( self , num_rounds , num_simulations_per_round , batch_size = 100 , learning_rate = 0.0005 , validation_fraction = 0.1 , stop_after_epochs = 20 ) special Run SNL This runs SNL for num_rounds rounds, using num_simulations_per_round calls to the simulator Parameters: Name Type Description Default num_rounds int Number of rounds to run required num_simulations_per_round Union[List[int], int] Number of simulator calls per round required batch_size int Size of batch to use for training. 100 learning_rate float Learning rate for Adam optimizer. 0.0005 validation_fraction float The fraction of data to use for validation. 0.1 stop_after_epochs int The number of epochs to wait for improvement on the validation set before terminating training. 20 Returns: Type Description Posterior Posterior that can be sampled and evaluated Source code in sbi/inference/snl/snl.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def __call__ ( self , num_rounds : int , num_simulations_per_round : Union [ List [ int ], int ], batch_size : int = 100 , learning_rate : float = 5e-4 , validation_fraction : float = 0.1 , stop_after_epochs : int = 20 , ) -> Posterior : \"\"\"Run SNL This runs SNL for num_rounds rounds, using num_simulations_per_round calls to the simulator Args: num_rounds: Number of rounds to run num_simulations_per_round: Number of simulator calls per round batch_size: Size of batch to use for training. learning_rate: Learning rate for Adam optimizer. validation_fraction: The fraction of data to use for validation. stop_after_epochs: The number of epochs to wait for improvement on the validation set before terminating training. Returns: Posterior that can be sampled and evaluated \"\"\" round_description = \"\" tbar = tqdm ( range ( num_rounds )) for round_ in tbar : tbar . set_description ( round_description ) # Generate parameters from prior in first round, and from most recent posterior # estimate in subsequent rounds. if round_ == 0 : parameters , observations = simulators . simulate_in_batches ( simulator = self . _simulator , parameter_sample_fn = lambda num_samples : self . _prior . sample ( ( num_samples ,) ), num_samples = num_simulations_per_round , simulation_batch_size = self . _simulation_batch_size , x_dim = self . _true_observation . shape [ 1 :], # do not pass batch_dim ) else : parameters , observations = simulators . simulate_in_batches ( simulator = self . _simulator , parameter_sample_fn = lambda num_samples : self . _neural_posterior . sample ( num_samples ), num_samples = num_simulations_per_round , simulation_batch_size = self . _simulation_batch_size , x_dim = self . _true_observation . shape [ 1 :], # do not pass batch_dim ) # Store (parameter, observation) pairs. self . _parameter_bank . append ( torch . as_tensor ( parameters )) self . _observation_bank . append ( torch . as_tensor ( observations )) # Fit neural likelihood to newly aggregated dataset. self . _train ( batch_size = batch_size , learning_rate = learning_rate , validation_fraction = validation_fraction , stop_after_epochs = stop_after_epochs , ) # Update description for progress bar. round_description = ( f \"------------------------- \\n \" f \"||||| ROUND { round_ + 1 } STATS |||||: \\n \" f \"------------------------- \\n \" f \"Epochs trained: { self . _summary [ 'epochs' ][ - 1 ] } \\n \" f \"Best validation performance: { self . _summary [ 'best_validation_log_probs' ][ - 1 ] : .4f } \\n\\n \" ) # Update TensorBoard and summary dict. self . _summary_writer , self . _summary = utils . summarize ( summary_writer = self . _summary_writer , summary = self . _summary , round_ = round_ , true_observation = self . _true_observation , parameter_bank = self . _parameter_bank , observation_bank = self . _observation_bank , simulator = self . _simulator , ) self . _neural_posterior . _num_trained_rounds = num_rounds return self . _neural_posterior __init__ ( self , simulator , prior , true_observation , density_estimator , simulation_batch_size = 1 , summary_writer = None , device = None , mcmc_method = 'slice-np' ) special Sequential Neural Likelihood Implementation of Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows by Papamakarios et al., AISTATS 2019, https://arxiv.org/abs/1805.07226 Parameters: Name Type Description Default density_estimator Optional[nn.Module] Conditional density estimator q(x|\\theta) q(x|\\theta) , a nn.Module with log_prob and sample methods required Source code in sbi/inference/snl/snl.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def __init__ ( self , simulator : Callable , prior , true_observation : Tensor , density_estimator : Optional [ nn . Module ], simulation_batch_size : int = 1 , summary_writer : SummaryWriter = None , device : torch . device = None , mcmc_method : str = \"slice-np\" , ): r \"\"\"Sequential Neural Likelihood Implementation of _Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows_ by Papamakarios et al., AISTATS 2019, https://arxiv.org/abs/1805.07226 Args: density_estimator: Conditional density estimator $q(x|\\theta)$, a nn.Module with `log_prob` and `sample` methods \"\"\" super () . __init__ ( simulator , prior , true_observation , simulation_batch_size , device , summary_writer , ) if density_estimator is None : density_estimator = utils . likelihood_nn ( model = \"maf\" , prior = self . _prior , context = self . _true_observation , ) # create neural posterior which can sample() self . _neural_posterior = Posterior ( algorithm_family = \"snl\" , neural_net = density_estimator , prior = prior , context = true_observation , mcmc_method = mcmc_method , get_potential_function = PotentialFunctionProvider (), ) # XXX why not density_estimator.train(True)??? self . _neural_posterior . neural_net . train ( True ) # SNL-specific summary_writer fields self . _summary . update ({ \"mcmc_times\" : []}) sbi.inference.sre.sre.SRE \u00b6 __call__ ( self , num_rounds , num_simulations_per_round , batch_size = 100 , learning_rate = 0.0005 , validation_fraction = 0.1 , stop_after_epochs = 20 ) special Run SRE This runs SRE for num_rounds rounds, using num_simulations_per_round calls to the simulator Parameters: Name Type Description Default num_rounds int Number of rounds to run required num_simulations_per_round Union[List[int], int] Number of simulator calls per round required batch_size int Size of batch to use for training. 100 learning_rate float Learning rate for Adam optimizer. 0.0005 validation_fraction float The fraction of data to use for validation. 0.1 stop_after_epochs int The number of epochs to wait for improvement on the validation set before terminating training. 20 Returns: Posterior that can be sampled and evaluated. Source code in sbi/inference/sre/sre.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 def __call__ ( self , num_rounds : int , num_simulations_per_round : Union [ List [ int ], int ], batch_size : int = 100 , learning_rate : float = 5e-4 , validation_fraction : float = 0.1 , stop_after_epochs : int = 20 , ) -> Posterior : \"\"\"Run SRE This runs SRE for num_rounds rounds, using num_simulations_per_round calls to the simulator Args: num_rounds: Number of rounds to run num_simulations_per_round: Number of simulator calls per round batch_size: Size of batch to use for training. learning_rate: Learning rate for Adam optimizer. validation_fraction: The fraction of data to use for validation. stop_after_epochs: The number of epochs to wait for improvement on the validation set before terminating training. Returns: Posterior that can be sampled and evaluated. \"\"\" round_description = \"\" tbar = tqdm ( range ( num_rounds )) for round_ in tbar : tbar . set_description ( round_description ) # Generate parameters from prior in first round, and from most recent posterior # estimate in subsequent rounds. if round_ == 0 : parameters , observations = simulators . simulate_in_batches ( simulator = self . _simulator , parameter_sample_fn = lambda num_samples : self . _prior . sample ( ( num_samples ,) ), num_samples = num_simulations_per_round , simulation_batch_size = self . _simulation_batch_size , x_dim = self . _true_observation . shape [ 1 :], # do not pass batch_dim ) else : parameters , observations = simulators . simulate_in_batches ( simulator = self . _simulator , parameter_sample_fn = lambda num_samples : self . _neural_posterior . sample ( num_samples ), num_samples = num_simulations_per_round , simulation_batch_size = self . _simulation_batch_size , x_dim = self . _true_observation . shape [ 1 :], # do not pass batch_dim ) # Store (parameter, observation) pairs. self . _parameter_bank . append ( torch . Tensor ( parameters )) self . _observation_bank . append ( torch . Tensor ( observations )) # Fit posterior using newly aggregated data set. self . _train ( batch_size = batch_size , learning_rate = learning_rate , validation_fraction = validation_fraction , stop_after_epochs = stop_after_epochs , ) # Update description for progress bar. round_description = ( f \"------------------------- \\n \" f \"||||| ROUND { round_ + 1 } STATS |||||: \\n \" f \"------------------------- \\n \" f \"Epochs trained: { self . _summary [ 'epochs' ][ - 1 ] } \\n \" f \"Best validation performance: { self . _summary [ 'best_validation_log_probs' ][ - 1 ] : .4f } \\n\\n \" ) # Update tensorboard and summary dict. self . _summary_writer , self . _summary = utils . summarize ( summary_writer = self . _summary_writer , summary = self . _summary , round_ = round_ , true_observation = self . _true_observation , parameter_bank = self . _parameter_bank , observation_bank = self . _observation_bank , simulator = self . _simulator , ) self . _neural_posterior . _num_trained_rounds = num_rounds return self . _neural_posterior __init__ ( self , simulator , prior , true_observation , classifier , num_atoms =- 1 , simulation_batch_size = 1 , mcmc_method = 'slice-np' , summary_net = None , classifier_loss = 'sre' , retrain_from_scratch_each_round = False , summary_writer = None , device = None ) special Sequential Ratio Estimation As presented in Likelihood-free MCMC with Amortized Approximate Likelihood Ratios by Hermans et al., Pre-print 2019, https://arxiv.org/abs/1903.04057 See NeuralInference docstring for all other arguments. Parameters: Name Type Description Default classifier nn.Module Binary classifier required num_atoms int Number of atoms to use for classification. If -1, use all other parameters in minibatch -1 retrain_from_scratch_each_round bool whether to retrain from scratch each round False summary_net Optional[nn.Module] Optional network which may be used to produce feature vectors f(x) for high-dimensional observations None classifier_loss str sre implements the algorithm suggested in Durkan et al. 2019, whereas aalr implements the algorithm suggested in Hermans et al. 2019. sre can use more than two atoms, potentially boosting performance, but does not allow for exact posterior density evaluation (only up to a normalizing constant), even when training only one round. aalr is limited to num_atoms=2 , but allows for density evaluation when training for one round. 'sre' Source code in sbi/inference/sre/sre.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def __init__ ( self , simulator : Callable , prior , true_observation : Tensor , classifier : nn . Module , num_atoms : int = - 1 , simulation_batch_size : int = 1 , mcmc_method : str = \"slice-np\" , summary_net : Optional [ nn . Module ] = None , classifier_loss : str = \"sre\" , retrain_from_scratch_each_round : bool = False , summary_writer : Optional [ SummaryWriter ] = None , device : Optional [ torch . device ] = None , ): \"\"\"Sequential Ratio Estimation As presented in _Likelihood-free MCMC with Amortized Approximate Likelihood Ratios_ by Hermans et al., Pre-print 2019, https://arxiv.org/abs/1903.04057 See NeuralInference docstring for all other arguments. Args: classifier: Binary classifier num_atoms: Number of atoms to use for classification. If -1, use all other parameters in minibatch retrain_from_scratch_each_round: whether to retrain from scratch each round summary_net: Optional network which may be used to produce feature vectors f(x) for high-dimensional observations classifier_loss: `sre` implements the algorithm suggested in Durkan et al. 2019, whereas `aalr` implements the algorithm suggested in Hermans et al. 2019. `sre` can use more than two atoms, potentially boosting performance, but does not allow for exact posterior density evaluation (only up to a normalizing constant), even when training only one round. `aalr` is limited to `num_atoms=2`, but allows for density evaluation when training for one round. \"\"\" super () . __init__ ( simulator , prior , true_observation , simulation_batch_size , device , summary_writer , ) self . _classifier_loss = classifier_loss assert isinstance ( num_atoms , int ), \"Number of atoms must be an integer.\" self . _num_atoms = num_atoms if classifier is None : classifier = utils . classifier_nn ( model = \"resnet\" , prior = self . _prior , context = self . _true_observation , ) # create posterior object which can sample() self . _neural_posterior = Posterior ( algorithm_family = self . _classifier_loss , neural_net = classifier , prior = prior , context = true_observation , mcmc_method = mcmc_method , get_potential_function = PotentialFunctionProvider (), ) # XXX why not classifier.train(True)??? self . _neural_posterior . neural_net . train ( True ) # We may want to summarize high-dimensional observations. # This may be either a fixed or learned transformation. if summary_net is None : self . _summary_net = nn . Identity () else : self . _summary_net = summary_net # If we're retraining from scratch each round, # keep a copy of the original untrained model for reinitialization. self . _retrain_from_scratch_each_round = retrain_from_scratch_each_round if self . _retrain_from_scratch_each_round : self . _untrained_classifier = deepcopy ( classifier ) else : self . _untrained_classifier = None # SRE-specific summary_writer fields self . _summary . update ({ \"mcmc_times\" : []}) Models \u00b6 sbi.utils.get_nn_models.posterior_nn ( model , prior , context , embedding = None , hidden_features = 50 , mdn_num_components = 20 , made_num_mixture_components = 10 , made_num_blocks = 4 , flow_num_transforms = 5 ) \u00b6 Neural posterior density estimator Parameters: Name Type Description Default model str Model, one of maf / mdn / made / nsf required prior torch.distributions.Distribution Prior distribution required context torch.Tensor Observation required embedding Optional[torch.nn.Module] Embedding network None hidden_features int For all, number of hidden features 50 mdn_num_components int For MDNs only, number of components 20 made_num_mixture_components int For MADEs only, number of mixture components 10 made_num_blocks int For MADEs only, number of blocks 4 flow_num_transforms int For flows only, number of transforms 5 Returns: Type Description torch.nn.Module Neural network Source code in sbi/utils/get_nn_models.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def posterior_nn ( model : str , prior : torch . distributions . Distribution , context : torch . Tensor , embedding : Optional [ torch . nn . Module ] = None , hidden_features : int = 50 , mdn_num_components : int = 20 , made_num_mixture_components : int = 10 , made_num_blocks : int = 4 , flow_num_transforms : int = 5 , ) -> torch . nn . Module : \"\"\"Neural posterior density estimator Args: model: Model, one of maf / mdn / made / nsf prior: Prior distribution context: Observation embedding: Embedding network hidden_features: For all, number of hidden features mdn_num_components: For MDNs only, number of components made_num_mixture_components: For MADEs only, number of mixture components made_num_blocks: For MADEs only, number of blocks flow_num_transforms: For flows only, number of transforms Returns: Neural network \"\"\" mean , std = ( prior . mean , prior . stddev ) standardizing_transform = transforms . AffineTransform ( shift =- mean / std , scale = 1 / std ) parameter_dim = prior . sample ([ 1 ]) . shape [ 1 ] context = utils . torchutils . atleast_2d ( context ) observation_dim = torch . tensor ([ context . shape [ 1 :]]) if model == \"mdn\" : neural_net = MultivariateGaussianMDN ( features = parameter_dim , context_features = observation_dim , hidden_features = hidden_features , hidden_net = nn . Sequential ( nn . Linear ( observation_dim , hidden_features ), nn . ReLU (), nn . Dropout ( p = 0.0 ), nn . Linear ( hidden_features , hidden_features ), nn . ReLU (), nn . Linear ( hidden_features , hidden_features ), nn . ReLU (), ), num_components = mdn_num_components , custom_initialization = True , ) elif model == \"made\" : transform = standardizing_transform distribution = distributions_ . MADEMoG ( features = parameter_dim , hidden_features = hidden_features , context_features = observation_dim , num_blocks = made_num_blocks , num_mixture_components = made_num_mixture_components , use_residual_blocks = True , random_mask = False , activation = torch . relu , dropout_probability = 0.0 , use_batch_norm = False , custom_initialization = True , ) neural_net = flows . Flow ( transform , distribution , embedding ) elif model == \"maf\" : transform = transforms . CompositeTransform ( [ transforms . CompositeTransform ( [ transforms . MaskedAffineAutoregressiveTransform ( features = parameter_dim , hidden_features = hidden_features , context_features = observation_dim , num_blocks = 2 , use_residual_blocks = False , random_mask = False , activation = torch . tanh , dropout_probability = 0.0 , use_batch_norm = True , ), transforms . RandomPermutation ( features = parameter_dim ), ] ) for _ in range ( flow_num_transforms ) ] ) transform = transforms . CompositeTransform ([ standardizing_transform , transform ,]) distribution = distributions_ . StandardNormal (( parameter_dim ,)) neural_net = flows . Flow ( transform , distribution , embedding ) elif model == \"nsf\" : transform = transforms . CompositeTransform ( [ transforms . CompositeTransform ( [ transforms . PiecewiseRationalQuadraticCouplingTransform ( mask = create_alternating_binary_mask ( features = parameter_dim , even = ( i % 2 == 0 ) ), transform_net_create_fn = lambda in_features , out_features : nets . ResidualNet ( in_features = in_features , out_features = out_features , hidden_features = hidden_features , context_features = observation_dim , num_blocks = 2 , activation = torch . relu , dropout_probability = 0.0 , use_batch_norm = False , ), num_bins = 10 , tails = \"linear\" , tail_bound = 3.0 , apply_unconditional_transform = False , ), transforms . LULinear ( parameter_dim , identity_init = True ), ] ) for i in range ( flow_num_transforms ) ] ) transform = transforms . CompositeTransform ([ standardizing_transform , transform ,]) distribution = distributions_ . StandardNormal (( parameter_dim ,)) neural_net = flows . Flow ( transform , distribution , embedding ) else : raise ValueError return neural_net sbi.utils.get_nn_models.likelihood_nn ( model , prior , context , embedding = None , hidden_features = 50 , mdn_num_components = 20 , made_num_mixture_components = 10 , made_num_blocks = 4 , flow_num_transforms = 5 ) \u00b6 Neural likelihood density estimator Parameters: Name Type Description Default model str Model, one of maf / mdn / made / nsf required prior torch.distributions.Distribution Prior distribution required context torch.Tensor Observation required embedding Optional[torch.nn.Module] Embedding network None hidden_features int For all, number of hidden features 50 mdn_num_components int For MDNs only, number of components 20 made_num_mixture_components int For MADEs only, number of mixture components 10 made_num_blocks int For MADEs only, number of blocks 4 flow_num_transforms int For flows only, number of transforms 5 Returns: Type Description torch.nn.Module Neural network Source code in sbi/utils/get_nn_models.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 def likelihood_nn ( model : str , prior : torch . distributions . Distribution , context : torch . Tensor , embedding : Optional [ torch . nn . Module ] = None , hidden_features : int = 50 , mdn_num_components : int = 20 , made_num_mixture_components : int = 10 , made_num_blocks : int = 4 , flow_num_transforms : int = 5 , ) -> torch . nn . Module : \"\"\"Neural likelihood density estimator Args: model: Model, one of maf / mdn / made / nsf prior: Prior distribution context: Observation embedding: Embedding network hidden_features: For all, number of hidden features mdn_num_components: For MDNs only, number of components made_num_mixture_components: For MADEs only, number of mixture components made_num_blocks: For MADEs only, number of blocks flow_num_transforms: For flows only, number of transforms Returns: Neural network \"\"\" parameter_dim = prior . sample ([ 1 ]) . shape [ 1 ] if context . dim () == 1 : observation_dim = torch . tensor ([ context . shape ]) . item () else : observation_dim = torch . tensor ([ context . shape [ 1 ]]) . item () if model == \"mdn\" : neural_net = MultivariateGaussianMDN ( features = observation_dim , context_features = parameter_dim , hidden_features = hidden_features , hidden_net = nn . Sequential ( nn . Linear ( parameter_dim , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), nn . Dropout ( p = 0.0 ), nn . Linear ( hidden_features , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), nn . Linear ( hidden_features , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), ), num_components = mdn_num_components , custom_initialization = True , ) elif model == \"made\" : neural_net = MixtureOfGaussiansMADE ( features = observation_dim , hidden_features = hidden_features , context_features = parameter_dim , num_blocks = made_num_blocks , num_mixture_components = made_num_mixture_components , use_residual_blocks = True , random_mask = False , activation = torch . relu , use_batch_norm = True , dropout_probability = 0.0 , custom_initialization = True , ) elif model == \"maf\" : transform = transforms . CompositeTransform ( [ transforms . CompositeTransform ( [ transforms . MaskedAffineAutoregressiveTransform ( features = observation_dim , hidden_features = hidden_features , context_features = parameter_dim , num_blocks = 2 , use_residual_blocks = False , random_mask = False , activation = torch . tanh , dropout_probability = 0.0 , use_batch_norm = True , ), transforms . RandomPermutation ( features = observation_dim ), ] ) for _ in range ( flow_num_transforms ) ] ) distribution = distributions_ . StandardNormal (( observation_dim ,)) neural_net = flows . Flow ( transform , distribution , embedding ) elif model == \"nsf\" : transform = transforms . CompositeTransform ( [ transforms . CompositeTransform ( [ transforms . PiecewiseRationalQuadraticCouplingTransform ( mask = create_alternating_binary_mask ( features = observation_dim , even = ( i % 2 == 0 ) ), transform_net_create_fn = lambda in_features , out_features : nets . ResidualNet ( in_features = in_features , out_features = out_features , hidden_features = hidden_features , context_features = parameter_dim , num_blocks = 2 , activation = torch . relu , dropout_probability = 0.0 , use_batch_norm = False , ), num_bins = 10 , tails = \"linear\" , tail_bound = 3.0 , apply_unconditional_transform = False , ), transforms . LULinear ( observation_dim , identity_init = True ), ] ) for i in range ( flow_num_transforms ) ] ) distribution = distributions_ . StandardNormal (( observation_dim ,)) neural_net = flows . Flow ( transform , distribution ) else : raise ValueError return neural_net sbi.utils.get_nn_models.classifier_nn ( model , prior , context , hidden_features = 50 ) \u00b6 Neural classifier Parameters: Name Type Description Default model Model, one of linear / mlp / resnet required prior torch.distributions.Distribution Prior distribution required context torch.Tensor Observation required hidden_features int For all, number of hidden features 50 Returns: Type Description torch.nn.Module Neural network Source code in sbi/utils/get_nn_models.py 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 def classifier_nn ( model , prior : torch . distributions . Distribution , context : torch . Tensor , hidden_features : int = 50 , ) -> torch . nn . Module : \"\"\"Neural classifier Args: model: Model, one of linear / mlp / resnet prior: Prior distribution context: Observation hidden_features: For all, number of hidden features Returns: Neural network \"\"\" parameter_dim = prior . sample ([ 1 ]) . shape [ 1 ] context = utils . torchutils . atleast_2d ( context ) observation_dim = torch . tensor ([ context . shape [ 1 :]]) if model == \"linear\" : neural_net = nn . Linear ( parameter_dim + observation_dim , 1 ) elif model == \"mlp\" : neural_net = nn . Sequential ( nn . Linear ( parameter_dim + observation_dim , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), nn . Linear ( hidden_features , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), nn . Linear ( hidden_features , 1 ), ) elif model == \"resnet\" : neural_net = nets . ResidualNet ( in_features = parameter_dim + observation_dim , out_features = 1 , hidden_features = hidden_features , context_features = None , num_blocks = 2 , activation = torch . relu , dropout_probability = 0.0 , use_batch_norm = False , ) else : raise ValueError ( f \"'model' must be one of ['linear', 'mlp', 'resnet'].\" ) return neural_net","title":"API Reference"},{"location":"reference/#api-reference","text":"","title":"API Reference"},{"location":"reference/#inference","text":"","title":"Inference"},{"location":"reference/#sbi.inference.snpe.snpe_a.SnpeA","text":"","title":"SnpeA"},{"location":"reference/#sbi.inference.snpe.snpe_b.SnpeB","text":"","title":"SnpeB"},{"location":"reference/#sbi.inference.snpe.snpe_c.SnpeC","text":"","title":"SnpeC"},{"location":"reference/#sbi.inference.snl.snl.SNL","text":"","title":"SNL"},{"location":"reference/#sbi.inference.sre.sre.SRE","text":"","title":"SRE"},{"location":"reference/#models","text":"","title":"Models"},{"location":"reference/#sbi.utils.get_nn_models.posterior_nn","text":"Neural posterior density estimator Parameters: Name Type Description Default model str Model, one of maf / mdn / made / nsf required prior torch.distributions.Distribution Prior distribution required context torch.Tensor Observation required embedding Optional[torch.nn.Module] Embedding network None hidden_features int For all, number of hidden features 50 mdn_num_components int For MDNs only, number of components 20 made_num_mixture_components int For MADEs only, number of mixture components 10 made_num_blocks int For MADEs only, number of blocks 4 flow_num_transforms int For flows only, number of transforms 5 Returns: Type Description torch.nn.Module Neural network Source code in sbi/utils/get_nn_models.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def posterior_nn ( model : str , prior : torch . distributions . Distribution , context : torch . Tensor , embedding : Optional [ torch . nn . Module ] = None , hidden_features : int = 50 , mdn_num_components : int = 20 , made_num_mixture_components : int = 10 , made_num_blocks : int = 4 , flow_num_transforms : int = 5 , ) -> torch . nn . Module : \"\"\"Neural posterior density estimator Args: model: Model, one of maf / mdn / made / nsf prior: Prior distribution context: Observation embedding: Embedding network hidden_features: For all, number of hidden features mdn_num_components: For MDNs only, number of components made_num_mixture_components: For MADEs only, number of mixture components made_num_blocks: For MADEs only, number of blocks flow_num_transforms: For flows only, number of transforms Returns: Neural network \"\"\" mean , std = ( prior . mean , prior . stddev ) standardizing_transform = transforms . AffineTransform ( shift =- mean / std , scale = 1 / std ) parameter_dim = prior . sample ([ 1 ]) . shape [ 1 ] context = utils . torchutils . atleast_2d ( context ) observation_dim = torch . tensor ([ context . shape [ 1 :]]) if model == \"mdn\" : neural_net = MultivariateGaussianMDN ( features = parameter_dim , context_features = observation_dim , hidden_features = hidden_features , hidden_net = nn . Sequential ( nn . Linear ( observation_dim , hidden_features ), nn . ReLU (), nn . Dropout ( p = 0.0 ), nn . Linear ( hidden_features , hidden_features ), nn . ReLU (), nn . Linear ( hidden_features , hidden_features ), nn . ReLU (), ), num_components = mdn_num_components , custom_initialization = True , ) elif model == \"made\" : transform = standardizing_transform distribution = distributions_ . MADEMoG ( features = parameter_dim , hidden_features = hidden_features , context_features = observation_dim , num_blocks = made_num_blocks , num_mixture_components = made_num_mixture_components , use_residual_blocks = True , random_mask = False , activation = torch . relu , dropout_probability = 0.0 , use_batch_norm = False , custom_initialization = True , ) neural_net = flows . Flow ( transform , distribution , embedding ) elif model == \"maf\" : transform = transforms . CompositeTransform ( [ transforms . CompositeTransform ( [ transforms . MaskedAffineAutoregressiveTransform ( features = parameter_dim , hidden_features = hidden_features , context_features = observation_dim , num_blocks = 2 , use_residual_blocks = False , random_mask = False , activation = torch . tanh , dropout_probability = 0.0 , use_batch_norm = True , ), transforms . RandomPermutation ( features = parameter_dim ), ] ) for _ in range ( flow_num_transforms ) ] ) transform = transforms . CompositeTransform ([ standardizing_transform , transform ,]) distribution = distributions_ . StandardNormal (( parameter_dim ,)) neural_net = flows . Flow ( transform , distribution , embedding ) elif model == \"nsf\" : transform = transforms . CompositeTransform ( [ transforms . CompositeTransform ( [ transforms . PiecewiseRationalQuadraticCouplingTransform ( mask = create_alternating_binary_mask ( features = parameter_dim , even = ( i % 2 == 0 ) ), transform_net_create_fn = lambda in_features , out_features : nets . ResidualNet ( in_features = in_features , out_features = out_features , hidden_features = hidden_features , context_features = observation_dim , num_blocks = 2 , activation = torch . relu , dropout_probability = 0.0 , use_batch_norm = False , ), num_bins = 10 , tails = \"linear\" , tail_bound = 3.0 , apply_unconditional_transform = False , ), transforms . LULinear ( parameter_dim , identity_init = True ), ] ) for i in range ( flow_num_transforms ) ] ) transform = transforms . CompositeTransform ([ standardizing_transform , transform ,]) distribution = distributions_ . StandardNormal (( parameter_dim ,)) neural_net = flows . Flow ( transform , distribution , embedding ) else : raise ValueError return neural_net","title":"posterior_nn()"},{"location":"reference/#sbi.utils.get_nn_models.likelihood_nn","text":"Neural likelihood density estimator Parameters: Name Type Description Default model str Model, one of maf / mdn / made / nsf required prior torch.distributions.Distribution Prior distribution required context torch.Tensor Observation required embedding Optional[torch.nn.Module] Embedding network None hidden_features int For all, number of hidden features 50 mdn_num_components int For MDNs only, number of components 20 made_num_mixture_components int For MADEs only, number of mixture components 10 made_num_blocks int For MADEs only, number of blocks 4 flow_num_transforms int For flows only, number of transforms 5 Returns: Type Description torch.nn.Module Neural network Source code in sbi/utils/get_nn_models.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 def likelihood_nn ( model : str , prior : torch . distributions . Distribution , context : torch . Tensor , embedding : Optional [ torch . nn . Module ] = None , hidden_features : int = 50 , mdn_num_components : int = 20 , made_num_mixture_components : int = 10 , made_num_blocks : int = 4 , flow_num_transforms : int = 5 , ) -> torch . nn . Module : \"\"\"Neural likelihood density estimator Args: model: Model, one of maf / mdn / made / nsf prior: Prior distribution context: Observation embedding: Embedding network hidden_features: For all, number of hidden features mdn_num_components: For MDNs only, number of components made_num_mixture_components: For MADEs only, number of mixture components made_num_blocks: For MADEs only, number of blocks flow_num_transforms: For flows only, number of transforms Returns: Neural network \"\"\" parameter_dim = prior . sample ([ 1 ]) . shape [ 1 ] if context . dim () == 1 : observation_dim = torch . tensor ([ context . shape ]) . item () else : observation_dim = torch . tensor ([ context . shape [ 1 ]]) . item () if model == \"mdn\" : neural_net = MultivariateGaussianMDN ( features = observation_dim , context_features = parameter_dim , hidden_features = hidden_features , hidden_net = nn . Sequential ( nn . Linear ( parameter_dim , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), nn . Dropout ( p = 0.0 ), nn . Linear ( hidden_features , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), nn . Linear ( hidden_features , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), ), num_components = mdn_num_components , custom_initialization = True , ) elif model == \"made\" : neural_net = MixtureOfGaussiansMADE ( features = observation_dim , hidden_features = hidden_features , context_features = parameter_dim , num_blocks = made_num_blocks , num_mixture_components = made_num_mixture_components , use_residual_blocks = True , random_mask = False , activation = torch . relu , use_batch_norm = True , dropout_probability = 0.0 , custom_initialization = True , ) elif model == \"maf\" : transform = transforms . CompositeTransform ( [ transforms . CompositeTransform ( [ transforms . MaskedAffineAutoregressiveTransform ( features = observation_dim , hidden_features = hidden_features , context_features = parameter_dim , num_blocks = 2 , use_residual_blocks = False , random_mask = False , activation = torch . tanh , dropout_probability = 0.0 , use_batch_norm = True , ), transforms . RandomPermutation ( features = observation_dim ), ] ) for _ in range ( flow_num_transforms ) ] ) distribution = distributions_ . StandardNormal (( observation_dim ,)) neural_net = flows . Flow ( transform , distribution , embedding ) elif model == \"nsf\" : transform = transforms . CompositeTransform ( [ transforms . CompositeTransform ( [ transforms . PiecewiseRationalQuadraticCouplingTransform ( mask = create_alternating_binary_mask ( features = observation_dim , even = ( i % 2 == 0 ) ), transform_net_create_fn = lambda in_features , out_features : nets . ResidualNet ( in_features = in_features , out_features = out_features , hidden_features = hidden_features , context_features = parameter_dim , num_blocks = 2 , activation = torch . relu , dropout_probability = 0.0 , use_batch_norm = False , ), num_bins = 10 , tails = \"linear\" , tail_bound = 3.0 , apply_unconditional_transform = False , ), transforms . LULinear ( observation_dim , identity_init = True ), ] ) for i in range ( flow_num_transforms ) ] ) distribution = distributions_ . StandardNormal (( observation_dim ,)) neural_net = flows . Flow ( transform , distribution ) else : raise ValueError return neural_net","title":"likelihood_nn()"},{"location":"reference/#sbi.utils.get_nn_models.classifier_nn","text":"Neural classifier Parameters: Name Type Description Default model Model, one of linear / mlp / resnet required prior torch.distributions.Distribution Prior distribution required context torch.Tensor Observation required hidden_features int For all, number of hidden features 50 Returns: Type Description torch.nn.Module Neural network Source code in sbi/utils/get_nn_models.py 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 def classifier_nn ( model , prior : torch . distributions . Distribution , context : torch . Tensor , hidden_features : int = 50 , ) -> torch . nn . Module : \"\"\"Neural classifier Args: model: Model, one of linear / mlp / resnet prior: Prior distribution context: Observation hidden_features: For all, number of hidden features Returns: Neural network \"\"\" parameter_dim = prior . sample ([ 1 ]) . shape [ 1 ] context = utils . torchutils . atleast_2d ( context ) observation_dim = torch . tensor ([ context . shape [ 1 :]]) if model == \"linear\" : neural_net = nn . Linear ( parameter_dim + observation_dim , 1 ) elif model == \"mlp\" : neural_net = nn . Sequential ( nn . Linear ( parameter_dim + observation_dim , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), nn . Linear ( hidden_features , hidden_features ), nn . BatchNorm1d ( hidden_features ), nn . ReLU (), nn . Linear ( hidden_features , 1 ), ) elif model == \"resnet\" : neural_net = nets . ResidualNet ( in_features = parameter_dim + observation_dim , out_features = 1 , hidden_features = hidden_features , context_features = None , num_blocks = 2 , activation = torch . relu , dropout_probability = 0.0 , use_batch_norm = False , ) else : raise ValueError ( f \"'model' must be one of ['linear', 'mlp', 'resnet'].\" ) return neural_net","title":"classifier_nn()"}]}